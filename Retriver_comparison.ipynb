{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LFTLRSLyjr7"
      },
      "source": [
        "**Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnFovXzkylL2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b619dda3-da05-4522-9887-60f62a37ba69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.56.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install numpy pandas scikit-learn torch sentence-transformers nltk\n",
        "import nltk\n",
        "\n",
        "# Existing downloads\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Add this for new NLTK versions\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine Tunning**"
      ],
      "metadata": {
        "id": "YQBEAp2pllLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# 1Ô∏è‚É£ Install dependencies\n",
        "# -------------------------------\n",
        "!pip install transformers datasets faiss-gpu accelerate sentence-transformers tqdm -q\n",
        "\n",
        "# -------------------------------\n",
        "# 2Ô∏è‚É£ Imports\n",
        "# -------------------------------\n",
        "import random\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from datasets import load_dataset\n",
        "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer, DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -------------------------------\n",
        "# 3Ô∏è‚É£ Load SQuAD v1.1 (20k examples)\n",
        "# -------------------------------\n",
        "dataset = load_dataset(\"squad\")\n",
        "train_data = dataset['train'].select(range(20000))\n",
        "val_data   = dataset['validation'].select(range(500))\n",
        "print(f\"Training examples: {len(train_data)}, Validation examples: {len(val_data)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4Ô∏è‚É£ Prepare DPR-format examples\n",
        "# -------------------------------\n",
        "def prepare_dpr_format(dataset, num_negatives=1):\n",
        "    examples = []\n",
        "    all_contexts = [ex['context'] for ex in dataset]\n",
        "\n",
        "    for ex in dataset:\n",
        "        question = ex['question']\n",
        "        positive_ctx = ex['context']\n",
        "        negatives = random.sample([c for c in all_contexts if c != positive_ctx], num_negatives)\n",
        "        examples.append({\n",
        "            'question': question,\n",
        "            'positive_ctx': positive_ctx,\n",
        "            'negative_ctxs': negatives\n",
        "        })\n",
        "    return examples\n",
        "\n",
        "train_examples = prepare_dpr_format(train_data)\n",
        "val_examples = prepare_dpr_format(val_data)\n",
        "\n",
        "# -------------------------------\n",
        "# 5Ô∏è‚É£ Load DPR encoders & tokenizers\n",
        "# -------------------------------\n",
        "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\").to(device)\n",
        "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\").to(device)\n",
        "\n",
        "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6Ô∏è‚É£ Encode examples on GPU\n",
        "# -------------------------------\n",
        "def encode_examples(examples, q_tokenizer, ctx_tokenizer, max_length=512):\n",
        "    q_inputs = q_tokenizer(\n",
        "        [ex['question'] for ex in examples],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "    ctx_inputs = ctx_tokenizer(\n",
        "        [ex['positive_ctx'] for ex in examples],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "    return q_inputs, ctx_inputs\n",
        "\n",
        "train_q, train_ctx = encode_examples(train_examples, q_tokenizer, ctx_tokenizer)\n",
        "val_q, val_ctx = encode_examples(val_examples, q_tokenizer, ctx_tokenizer)\n",
        "\n",
        "# -------------------------------\n",
        "# 7Ô∏è‚É£ Fine-tuning loop on GPU\n",
        "# -------------------------------\n",
        "optimizer = optim.Adam(list(q_encoder.parameters()) + list(ctx_encoder.parameters()), lr=1e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "batch_size = 8\n",
        "num_epochs = 2\n",
        "gradient_accumulation_steps = 2\n",
        "\n",
        "num_batches = len(train_q['input_ids']) // batch_size + 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    q_encoder.train()\n",
        "    ctx_encoder.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    for i in tqdm(range(0, len(train_q['input_ids']), batch_size), total=num_batches, desc=\"Training\"):\n",
        "        q_batch = {k: v[i:i+batch_size].to(device) for k, v in train_q.items()}\n",
        "        ctx_batch = {k: v[i:i+batch_size].to(device) for k, v in train_ctx.items()}\n",
        "\n",
        "        q_emb = q_encoder(**q_batch).pooler_output\n",
        "        ctx_emb = ctx_encoder(**ctx_batch).pooler_output\n",
        "\n",
        "        scores = torch.matmul(q_emb, ctx_emb.T)\n",
        "        labels = torch.arange(scores.size(0)).to(device)\n",
        "        loss = loss_fn(scores, labels) / gradient_accumulation_steps\n",
        "        loss.backward()\n",
        "\n",
        "        if (i // batch_size + 1) % gradient_accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item() * gradient_accumulation_steps\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    print(f\"Epoch {epoch+1} average training loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # -------------------------------\n",
        "    # Validation on GPU\n",
        "    # -------------------------------\n",
        "    q_encoder.eval()\n",
        "    ctx_encoder.eval()\n",
        "    with torch.no_grad():\n",
        "        q_emb_val = q_encoder(**{k: v.to(device) for k, v in val_q.items()}).pooler_output\n",
        "        ctx_emb_val = ctx_encoder(**{k: v.to(device) for k, v in val_ctx.items()}).pooler_output\n",
        "        val_scores = torch.matmul(q_emb_val, ctx_emb_val.T)\n",
        "        val_labels = torch.arange(val_scores.size(0)).to(device)\n",
        "        val_loss = loss_fn(val_scores, val_labels).item()\n",
        "        print(f\"Epoch {epoch+1} validation loss: {val_loss:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 8Ô∏è‚É£ Save fine-tuned models\n",
        "# -------------------------------\n",
        "q_encoder.save_pretrained(\"/content/dpr_question_encoder_squad_20k_gpu\")\n",
        "ctx_encoder.save_pretrained(\"/content/dpr_ctx_encoder_squad_20k_gpu\")\n",
        "print(\"Fine-tuned DPR models saved successfully!\")\n",
        "\n",
        "# -------------------------------\n",
        "# 9Ô∏è‚É£ Example usage on GPU\n",
        "# -------------------------------\n",
        "query = \"What is machine learning?\"\n",
        "q_inputs = q_tokenizer(query, return_tensors=\"pt\").to(device)\n",
        "q_emb = q_encoder(**q_inputs).pooler_output\n",
        "print(\"Query embedding shape:\", q_emb.shape)\n"
      ],
      "metadata": {
        "id": "6ratZzrcllid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving in disk**"
      ],
      "metadata": {
        "id": "j_DK1Q8Kl3oF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# -------------------------------\n",
        "# 1Ô∏è‚É£ Mount Google Drive\n",
        "# -------------------------------\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# -------------------------------\n",
        "# 2Ô∏è‚É£ Define paths (corrected!)\n",
        "# -------------------------------\n",
        "q_encoder_path = \"/content/dpr_question_encoder_squad_20k_gpu\"\n",
        "ctx_encoder_path = \"/content/dpr_ctx_encoder_squad_20k_gpu\"\n",
        "\n",
        "drive_folder = \"/content/drive/MyDrive/DPR_Models\"\n",
        "os.makedirs(drive_folder, exist_ok=True)\n",
        "\n",
        "# -------------------------------\n",
        "# 3Ô∏è‚É£ Function to zip a folder safely\n",
        "# -------------------------------\n",
        "def zip_folder(src_path, dst_folder, zip_name):\n",
        "    if os.path.exists(src_path) and os.path.isdir(src_path):\n",
        "        zip_path = os.path.join(dst_folder, f\"{zip_name}.zip\")\n",
        "        shutil.make_archive(base_name=zip_path.replace('.zip', ''), format='zip', root_dir=src_path)\n",
        "        print(f\"{zip_name} successfully saved to: {zip_path}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Folder not found: {src_path}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4Ô∏è‚É£ Zip the models\n",
        "# -------------------------------\n",
        "zip_folder(q_encoder_path, drive_folder, \"dpr_question_encoder_squad_20k_gpu\")\n",
        "zip_folder(ctx_encoder_path, drive_folder, \"dpr_ctx_encoder_squad_20k_gpu\")\n"
      ],
      "metadata": {
        "id": "XEvN8f9Bl349"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGZ0eUvHxOi5"
      },
      "source": [
        "**Indexing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVOU6dsB3k-o"
      },
      "outputs": [],
      "source": [
        "NUM_TRAIN_EXAMPLES = 85000\n",
        "NUM_VALIDATION_EXAMPLES = 2000\n",
        "TOP_K = 7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGmP2tfF3lCH",
        "outputId": "68c3c25d-840f-4ade-a9e3-b68d27a42831"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading SQuAD dataset...\n",
            "Indexing BM25...\n",
            "Indexing fine-tuned DPR...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
            "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
            "Encoding DPR corpus: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [18:54<00:00,  4.41batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Hybrid Retriever...\n",
            "‚úÖ Retrievers indexed and saved to retrievers_v1.pkl\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# BM25 + Fine-Tuned DPR + Hybrid Retriever\n",
        "# ================================\n",
        "\n",
        "!pip install datasets scikit-learn nltk torch tqdm transformers --quiet\n",
        "\n",
        "import re, math, os, pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from datasets import load_dataset\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer, DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
        "\n",
        "# NLTK setup\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# ================================\n",
        "# CONFIG\n",
        "# ================================\n",
        "DATASET_SELECTION = \"v1\"\n",
        "#NUM_TRAIN_EXAMPLES = 200  # Adjust as needed\n",
        "PICKLE_FILE = f\"retrievers_{DATASET_SELECTION}.pkl\"\n",
        "#TOP_K = 5\n",
        "\n",
        "CTX_MODEL_PATH = \"/content/dpr_ctx_encoder_squad_20k_gpu\"\n",
        "Q_MODEL_PATH = \"/content/dpr_question_encoder_squad_20k_gpu\"\n",
        "\n",
        "# ================================\n",
        "# Text Preprocessor\n",
        "# ================================\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def preprocess_text(self, text: str) -> str:\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', str(text).lower())\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def tokenize_and_stem(self, text: str):\n",
        "        tokens = word_tokenize(self.preprocess_text(text))\n",
        "        return [self.stemmer.stem(t) for t in tokens if t not in self.stop_words and len(t) > 2]\n",
        "\n",
        "# ================================\n",
        "# BM25 Retriever\n",
        "# ================================\n",
        "class BM25Retriever:\n",
        "    def __init__(self, k1=1.5, b=0.75):\n",
        "        self.k1, self.b = k1, b\n",
        "        self.preprocessor = TextPreprocessor()\n",
        "\n",
        "    def fit(self, corpus: List[str]):\n",
        "        self.corpus = corpus\n",
        "        self.tokens = [self.preprocessor.tokenize_and_stem(doc) for doc in corpus]\n",
        "        df = defaultdict(int)\n",
        "        for tks in self.tokens:\n",
        "            for tok in set(tks):\n",
        "                df[tok] += 1\n",
        "        self.idf = {tok: math.log((len(corpus)-f+0.5)/(f+0.5)+1) for tok,f in df.items()}\n",
        "        self.doc_len = [len(tks) for tks in self.tokens]\n",
        "        self.avgdl = np.mean(self.doc_len)\n",
        "\n",
        "    def get_scores(self, query: str):\n",
        "        q_toks = self.preprocessor.tokenize_and_stem(query)\n",
        "        scores = np.zeros(len(self.corpus))\n",
        "        for i, doc in enumerate(self.tokens):\n",
        "            freqs = Counter(doc)\n",
        "            for tok in q_toks:\n",
        "                if tok in freqs:\n",
        "                    tf, idf = freqs[tok], self.idf.get(tok,0)\n",
        "                    denom = tf + self.k1*(1-self.b+self.b*self.doc_len[i]/self.avgdl)\n",
        "                    scores[i] += idf*(tf*(self.k1+1))/denom\n",
        "        return scores\n",
        "\n",
        "    def retrieve(self, query, k=TOP_K):\n",
        "        s = self.get_scores(query)\n",
        "        idx = np.argsort(s)[::-1][:k]\n",
        "        return [{'document': self.corpus[i], 'score': s[i]} for i in idx]\n",
        "\n",
        "# ================================\n",
        "# DPR Retriever (Fine-Tuned) - Fixed\n",
        "# ================================\n",
        "class DPRRetriever:\n",
        "    def __init__(self, ctx_model_path=CTX_MODEL_PATH, q_model_path=Q_MODEL_PATH, batch_size=16):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        # Tokenizers: original pretrained\n",
        "        self.ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\n",
        "            \"facebook/dpr-ctx_encoder-single-nq-base\"\n",
        "        )\n",
        "        self.q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\n",
        "            \"facebook/dpr-question_encoder-single-nq-base\"\n",
        "        )\n",
        "\n",
        "        # Encoders: fine-tuned weights\n",
        "        self.ctx_encoder = DPRContextEncoder.from_pretrained(ctx_model_path).to(self.device)\n",
        "        self.q_encoder = DPRQuestionEncoder.from_pretrained(q_model_path).to(self.device)\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.max_length = 512  # truncate passages to avoid size mismatch\n",
        "\n",
        "    def fit(self, corpus: List[str]):\n",
        "        self.corpus = corpus\n",
        "        all_embs = []\n",
        "        total_batches = math.ceil(len(corpus)/self.batch_size)\n",
        "        with tqdm(total=total_batches, desc=\"Encoding DPR corpus\", unit=\"batch\") as pbar:\n",
        "            for i in range(0, len(corpus), self.batch_size):\n",
        "                batch = corpus[i:i+self.batch_size]\n",
        "                inputs = self.ctx_tokenizer(\n",
        "                    batch,\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=self.max_length,\n",
        "                    return_tensors=\"pt\"\n",
        "                ).to(self.device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    emb = self.ctx_encoder(**inputs).pooler_output\n",
        "                all_embs.append(emb.cpu())\n",
        "                pbar.update(1)\n",
        "\n",
        "        self.embeddings = torch.cat(all_embs, dim=0)\n",
        "\n",
        "    def retrieve(self, query, k=TOP_K):\n",
        "        inputs = self.q_tokenizer(\n",
        "            [query],\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_emb = self.q_encoder(**inputs).pooler_output.cpu()\n",
        "\n",
        "        sims = cosine_similarity(q_emb.numpy(), self.embeddings.numpy())[0]\n",
        "        idx = np.argsort(sims)[::-1][:k]\n",
        "        return [{'document': self.corpus[i], 'score': sims[i]} for i in idx]\n",
        "\n",
        "# ================================\n",
        "# Hybrid Retriever\n",
        "# ================================\n",
        "class HybridRetriever:\n",
        "    def __init__(self, bm25: BM25Retriever, dpr: DPRRetriever, alpha: float = 0.5):\n",
        "        self.bm25 = bm25\n",
        "        self.dpr = dpr\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def retrieve(self, query, k=TOP_K):\n",
        "        bm25_res = self.bm25.retrieve(query, k=len(self.bm25.corpus))\n",
        "        dpr_res = self.dpr.retrieve(query, k=len(self.dpr.corpus))\n",
        "        combined_scores = {}\n",
        "        for r in bm25_res:\n",
        "            combined_scores[r['document']] = combined_scores.get(r['document'], 0) + (1-self.alpha)*r['score']\n",
        "        for r in dpr_res:\n",
        "            combined_scores[r['document']] = combined_scores.get(r['document'], 0) + self.alpha*r['score']\n",
        "        sorted_docs = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
        "        return [{'document': doc, 'score': score} for doc, score in sorted_docs]\n",
        "\n",
        "# ================================\n",
        "# Load / Index\n",
        "# ================================\n",
        "if os.path.exists(PICKLE_FILE):\n",
        "    with open(PICKLE_FILE, \"rb\") as f:\n",
        "        retrievers = pickle.load(f)\n",
        "    print(f\"‚úÖ Loaded retrievers from {PICKLE_FILE}. You can skip indexing.\")\n",
        "else:\n",
        "    print(\"Loading SQuAD dataset...\")\n",
        "    squad_train = load_dataset(\"squad\", split=f\"train[:{NUM_TRAIN_EXAMPLES}]\")\n",
        "    docs = [item['context'] for item in squad_train]\n",
        "\n",
        "    print(\"Indexing BM25...\")\n",
        "    bm25 = BM25Retriever()\n",
        "    bm25.fit(docs)\n",
        "\n",
        "    print(\"Indexing fine-tuned DPR...\")\n",
        "    dpr = DPRRetriever()\n",
        "    dpr.fit(docs)\n",
        "\n",
        "    print(\"Creating Hybrid Retriever...\")\n",
        "    hybrid = HybridRetriever(bm25, dpr, alpha=0.5)\n",
        "\n",
        "    retrievers = {\"bm25\": bm25, \"dpr\": dpr, \"hybrid\": hybrid}\n",
        "\n",
        "    with open(PICKLE_FILE, \"wb\") as f:\n",
        "        pickle.dump(retrievers, f)\n",
        "    print(f\"‚úÖ Retrievers indexed and saved to {PICKLE_FILE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlqwKexN0Fx3"
      },
      "source": [
        "**Retriver Only comparison**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHKu1XEy4hzN",
        "outputId": "e168663d-9ec5-4dd0-cc49-95e51f6e2448"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Retrievers loaded from retrievers_v1.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating queries:   0%|          | 4/2000 [00:21<2:54:19,  5.24s/it]"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# ================================\n",
        "# Configuration\n",
        "# ================================\n",
        "DATASET_SELECTION = \"v1\"\n",
        "#NUM_VALIDATION_EXAMPLES = 10\n",
        "#TOP_K = 5\n",
        "PICKLE_FILE = \"retrievers_v1.pkl\"\n",
        "SEMANTIC_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "SEM_SIM_THRESHOLD = 0.6  # Threshold for hit\n",
        "\n",
        "# ================================\n",
        "# Load retrievers\n",
        "# ================================\n",
        "with open(PICKLE_FILE, \"rb\") as f:\n",
        "    retrievers = pickle.load(f)\n",
        "print(f\"‚úÖ Retrievers loaded from {PICKLE_FILE}\")\n",
        "\n",
        "# ================================\n",
        "# Load validation data\n",
        "# ================================\n",
        "val_data = []\n",
        "if DATASET_SELECTION in [\"v1\", \"both\"]:\n",
        "    squad_v1_val = load_dataset(\"squad\", split=f\"validation[:{NUM_VALIDATION_EXAMPLES}]\")\n",
        "    val_data.extend([(\"v1\", item) for item in squad_v1_val])\n",
        "\n",
        "if DATASET_SELECTION in [\"v2\", \"both\"]:\n",
        "    squad_v2_val = load_dataset(\"squad_v2\", split=f\"validation[:{NUM_VALIDATION_EXAMPLES}]\")\n",
        "    val_data.extend([(\"v2\", item) for item in squad_v2_val])\n",
        "\n",
        "# ================================\n",
        "# Normalize scores function\n",
        "# ================================\n",
        "def normalize_scores(scores):\n",
        "    if len(scores) == 0:\n",
        "        return scores\n",
        "    min_s, max_s = min(scores), max(scores)\n",
        "    if max_s - min_s == 0:\n",
        "        return [0.0] * len(scores)\n",
        "    return [(s - min_s) / (max_s - min_s) for s in scores]\n",
        "\n",
        "# ================================\n",
        "# Load semantic similarity model\n",
        "# ================================\n",
        "semantic_model = SentenceTransformer(SEMANTIC_MODEL)\n",
        "\n",
        "# ================================\n",
        "# Evaluation\n",
        "# ================================\n",
        "results = []\n",
        "\n",
        "for dataset_name, example in tqdm(val_data, desc=\"Evaluating queries\"):\n",
        "    question = example['question']\n",
        "    gold_answer = example['answers']['text'][0] if example['answers']['text'] else \"\"\n",
        "\n",
        "    for retriever_type in [\"bm25\", \"dpr\", \"hybrid\"]:\n",
        "        retriever = retrievers.get(retriever_type)\n",
        "        if retriever is None:\n",
        "            continue\n",
        "\n",
        "        retrieved_docs = retriever.retrieve(question, k=TOP_K)\n",
        "        raw_scores = [r['score'] for r in retrieved_docs]\n",
        "        norm_scores = normalize_scores(raw_scores)\n",
        "\n",
        "        for rank, (r, score) in enumerate(zip(retrieved_docs, norm_scores), 1):\n",
        "            # Compute semantic similarity\n",
        "            sem_sim = cosine_similarity(\n",
        "                semantic_model.encode([gold_answer]),\n",
        "                semantic_model.encode([r['document']])\n",
        "            )[0][0]\n",
        "\n",
        "            # Hit if semantic similarity >= threshold\n",
        "            hit = 1 if sem_sim >= SEM_SIM_THRESHOLD else 0\n",
        "\n",
        "            results.append({\n",
        "                \"Dataset\": dataset_name,\n",
        "                \"Retriever\": retriever_type,\n",
        "                \"Question\": question,\n",
        "                \"Gold Answer\": gold_answer,\n",
        "                \"Rank\": rank,\n",
        "                \"Retrieved\": r['document'][:200],\n",
        "                \"Score\": score,\n",
        "                \"Hit\": hit,\n",
        "                \"Semantic Sim\": sem_sim\n",
        "            })\n",
        "\n",
        "# ================================\n",
        "# Convert results to DataFrame\n",
        "# ================================\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "# ================================\n",
        "# Compute metrics per retriever\n",
        "# ================================\n",
        "metrics = []\n",
        "for retriever_type in df_results[\"Retriever\"].unique():\n",
        "    df_r = df_results[df_results[\"Retriever\"] == retriever_type]\n",
        "    total_questions = df_r[\"Question\"].nunique()\n",
        "    hits_per_question = df_r.groupby(\"Question\")[\"Hit\"].max()\n",
        "\n",
        "    precision = df_r[\"Hit\"].sum() / len(df_r)\n",
        "    recall = hits_per_question.sum() / total_questions\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "    accuracy = recall\n",
        "    avg_sem_sim = df_r[\"Semantic Sim\"].mean()\n",
        "\n",
        "    metrics.append({\n",
        "        \"Retriever\": retriever_type,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1\": f1,\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Avg Semantic Sim\": avg_sem_sim\n",
        "    })\n",
        "\n",
        "df_metrics = pd.DataFrame(metrics)\n",
        "\n",
        "print(\"\\nüìä Retriever Metrics (F1 / Precision / Recall / Semantic Similarity):\")\n",
        "print(df_metrics)\n",
        "\n",
        "# ================================\n",
        "# Save results\n",
        "# ================================\n",
        "df_results.to_csv(\"retriever_results_semantic.csv\", index=False)\n",
        "df_metrics.to_csv(\"retriever_metrics_semantic.csv\", index=False)\n",
        "print(\"\\n‚úÖ Detailed results saved to CSV\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unzLJagc9XQT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OX6T-QpIHJby",
        "outputId": "f8e5f7dd-2152-43a6-ef01-816d2f85ff57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîé Testing retriever: bm25\n",
            "   Returned 5 results\n",
            "   ‚ûú {'document': \"All of Notre Dame's undergraduate students are a part of one of the five undergraduate colleges at the school or are in the First Year of Studies program. The First Year of Studies program was established in 1962 to guide incoming freshmen in their first year at the school before they have declared a major. Each student is given an academic advisor from the program who helps them to choose classes that give them exposure to any major in which they are interested. The program also includes a Learning Resource Center which provides time management, collaborative learning, and subject tutoring. This program has been recognized previously, by U.S. News & World Report, as outstanding.\", 'score': np.float64(5.987318973268757)}\n",
            "   ‚ûú {'document': \"All of Notre Dame's undergraduate students are a part of one of the five undergraduate colleges at the school or are in the First Year of Studies program. The First Year of Studies program was established in 1962 to guide incoming freshmen in their first year at the school before they have declared a major. Each student is given an academic advisor from the program who helps them to choose classes that give them exposure to any major in which they are interested. The program also includes a Learning Resource Center which provides time management, collaborative learning, and subject tutoring. This program has been recognized previously, by U.S. News & World Report, as outstanding.\", 'score': np.float64(5.987318973268757)}\n",
            "   ‚ûú {'document': \"All of Notre Dame's undergraduate students are a part of one of the five undergraduate colleges at the school or are in the First Year of Studies program. The First Year of Studies program was established in 1962 to guide incoming freshmen in their first year at the school before they have declared a major. Each student is given an academic advisor from the program who helps them to choose classes that give them exposure to any major in which they are interested. The program also includes a Learning Resource Center which provides time management, collaborative learning, and subject tutoring. This program has been recognized previously, by U.S. News & World Report, as outstanding.\", 'score': np.float64(5.987318973268757)}\n",
            "\n",
            "üîé Testing retriever: dpr\n",
            "   Returned 5 results\n",
            "   ‚ûú {'document': 'As of 2012[update] research continued in many fields. The university president, John Jenkins, described his hope that Notre Dame would become \"one of the pre‚Äìeminent research institutions in the world\" in his inaugural address. The university has many multi-disciplinary institutes devoted to research in varying fields, including the Medieval Institute, the Kellogg Institute for International Studies, the Kroc Institute for International Peace studies, and the Center for Social Concerns. Recent research includes work on family conflict and child development, genome mapping, the increasing trade deficit of the United States with China, studies in fluid mechanics, computational science and engineering, and marketing trends on the Internet. As of 2013, the university is home to the Notre Dame Global Adaptation Index which ranks countries annually based on how vulnerable they are to climate change and how prepared they are to adapt.', 'score': np.float32(0.74987763)}\n",
            "   ‚ûú {'document': 'As of 2012[update] research continued in many fields. The university president, John Jenkins, described his hope that Notre Dame would become \"one of the pre‚Äìeminent research institutions in the world\" in his inaugural address. The university has many multi-disciplinary institutes devoted to research in varying fields, including the Medieval Institute, the Kellogg Institute for International Studies, the Kroc Institute for International Peace studies, and the Center for Social Concerns. Recent research includes work on family conflict and child development, genome mapping, the increasing trade deficit of the United States with China, studies in fluid mechanics, computational science and engineering, and marketing trends on the Internet. As of 2013, the university is home to the Notre Dame Global Adaptation Index which ranks countries annually based on how vulnerable they are to climate change and how prepared they are to adapt.', 'score': np.float32(0.74987763)}\n",
            "   ‚ûú {'document': 'As of 2012[update] research continued in many fields. The university president, John Jenkins, described his hope that Notre Dame would become \"one of the pre‚Äìeminent research institutions in the world\" in his inaugural address. The university has many multi-disciplinary institutes devoted to research in varying fields, including the Medieval Institute, the Kellogg Institute for International Studies, the Kroc Institute for International Peace studies, and the Center for Social Concerns. Recent research includes work on family conflict and child development, genome mapping, the increasing trade deficit of the United States with China, studies in fluid mechanics, computational science and engineering, and marketing trends on the Internet. As of 2013, the university is home to the Notre Dame Global Adaptation Index which ranks countries annually based on how vulnerable they are to climate change and how prepared they are to adapt.', 'score': np.float32(0.74987763)}\n",
            "\n",
            "üîé Testing retriever: hybrid\n",
            "   Returned 5 results\n",
            "   ‚ûú {'document': \"All of Notre Dame's undergraduate students are a part of one of the five undergraduate colleges at the school or are in the First Year of Studies program. The First Year of Studies program was established in 1962 to guide incoming freshmen in their first year at the school before they have declared a major. Each student is given an academic advisor from the program who helps them to choose classes that give them exposure to any major in which they are interested. The program also includes a Learning Resource Center which provides time management, collaborative learning, and subject tutoring. This program has been recognized previously, by U.S. News & World Report, as outstanding.\", 'score': np.float64(13.384095749310037)}\n",
            "   ‚ûú {'document': 'As of 2012[update] research continued in many fields. The university president, John Jenkins, described his hope that Notre Dame would become \"one of the pre‚Äìeminent research institutions in the world\" in his inaugural address. The university has many multi-disciplinary institutes devoted to research in varying fields, including the Medieval Institute, the Kellogg Institute for International Studies, the Kroc Institute for International Peace studies, and the Center for Social Concerns. Recent research includes work on family conflict and child development, genome mapping, the increasing trade deficit of the United States with China, studies in fluid mechanics, computational science and engineering, and marketing trends on the Internet. As of 2013, the university is home to the Notre Dame Global Adaptation Index which ranks countries annually based on how vulnerable they are to climate change and how prepared they are to adapt.', 'score': np.float64(1.8746940791606903)}\n",
            "   ‚ûú {'document': \"In 1882, Albert Zahm (John Zahm's brother) built an early wind tunnel used to compare lift to drag of aeronautical models. Around 1899, Professor Jerome Green became the first American to send a wireless message. In 1931, Father Julius Nieuwland performed early work on basic reactions that was used to create neoprene. Study of nuclear physics at the university began with the building of a nuclear accelerator in 1936, and continues now partly through a partnership in the Joint Institute for Nuclear Astrophysics.\", 'score': np.float64(1.8276886641979218)}\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Load retrievers\n",
        "with open(\"/content/retrievers_v1.pkl\", \"rb\") as f:\n",
        "    retrievers = pickle.load(f)\n",
        "\n",
        "# Sample query\n",
        "query = \"What is machine learning?\"\n",
        "\n",
        "# Test each retriever\n",
        "for name, retriever in retrievers.items():\n",
        "    print(f\"\\nüîé Testing retriever: {name}\")\n",
        "    try:\n",
        "        results = retriever.retrieve(query)\n",
        "        print(f\"   Returned {len(results)} results\")\n",
        "        for r in results[:3]:  # show only top 3 results\n",
        "            print(\"   ‚ûú\", r)\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è Error running {name}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENGeJfjbHjKU",
        "outputId": "f28db761-fd99-4097-a9d0-2dda4ab4046a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bm25_v1: top result score = 13.621926586355245\n",
            "dpr_v1: top result score = 0.506264865398407\n",
            "hybrid_v1: top result score = 37.050485626545935\n"
          ]
        }
      ],
      "source": [
        "# Compare scores for the top result of each retriever\n",
        "for name, retriever in retrievers.items():\n",
        "    results = retriever.retrieve(query)\n",
        "    if results:\n",
        "        top_score = results[0]['score']\n",
        "        print(f\"{name}: top result score = {top_score}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LLM + Retriver**"
      ],
      "metadata": {
        "id": "U45rltTtmLWK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNvnXOxT40ZI"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# Install packages\n",
        "# ================================\n",
        "!pip install sentence-transformers datasets scikit-learn nltk torch tqdm transformers rouge-score sacrebleu --quiet\n",
        "\n",
        "import pickle, re, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "\n",
        "# NLTK setup\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# ================================\n",
        "# Configuration\n",
        "# ================================\n",
        "DATASET_SELECTION = \"v1\"\n",
        "#NUM_VALIDATION_EXAMPLES = 5   # small for testing\n",
        "#TOP_K = 5\n",
        "PICKLE_FILE = \"retrievers_v1.pkl\"\n",
        "LLM_MODEL = \"EleutherAI/gpt-neo-125M\"\n",
        "SEMANTIC_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "MAX_CONTEXT_TOKENS = 500  # limit tokens sent to LLM to avoid crashing\n",
        "\n",
        "# ================================\n",
        "# Text preprocessor (for BM25)\n",
        "# ================================\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "    def preprocess_text(self, text: str) -> str:\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', str(text).lower())\n",
        "        return ' '.join(text.split())\n",
        "    def tokenize_and_stem(self, text: str):\n",
        "        tokens = word_tokenize(self.preprocess_text(text))\n",
        "        return [self.stemmer.stem(t) for t in tokens if t not in self.stop_words and len(t) > 2]\n",
        "\n",
        "# ================================\n",
        "# Retriever Classes\n",
        "# ================================\n",
        "class BM25Retriever:\n",
        "    def __init__(self, k1=1.5, b=0.75):\n",
        "        self.k1, self.b = k1, b\n",
        "        self.preprocessor = TextPreprocessor()\n",
        "    def fit(self, corpus: List[str]):\n",
        "        self.corpus = corpus\n",
        "        self.tokens = [self.preprocessor.tokenize_and_stem(doc) for doc in corpus]\n",
        "        df = defaultdict(int)\n",
        "        for tks in self.tokens:\n",
        "            for tok in set(tks): df[tok] += 1\n",
        "        self.idf = {tok: math.log((len(corpus)-f+0.5)/(f+0.5)+1) for tok,f in df.items()}\n",
        "        self.doc_len = [len(tks) for tks in self.tokens]\n",
        "        self.avgdl = np.mean(self.doc_len)\n",
        "    def get_scores(self, query: str):\n",
        "        q_toks = self.preprocessor.tokenize_and_stem(query)\n",
        "        scores = np.zeros(len(self.corpus))\n",
        "        for i, doc in enumerate(self.tokens):\n",
        "            freqs = Counter(doc)\n",
        "            for tok in q_toks:\n",
        "                if tok in freqs:\n",
        "                    tf, idf = freqs[tok], self.idf.get(tok,0)\n",
        "                    denom = tf + self.k1*(1-self.b+self.b*self.doc_len[i]/self.avgdl)\n",
        "                    scores[i]+=idf*(tf*(self.k1+1))/denom\n",
        "        return scores\n",
        "    def retrieve(self, query, k=3):\n",
        "        s = self.get_scores(query)\n",
        "        idx = np.argsort(s)[::-1][:k]\n",
        "        return [{'document':self.corpus[i],'score':s[i]} for i in idx]\n",
        "\n",
        "class DPRRetriever:\n",
        "    def __init__(self, model_name=\"paraphrase-MiniLM-L3-v2\", batch_size=16):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.batch_size = batch_size\n",
        "    def fit(self, corpus: List[str]):\n",
        "        self.corpus = corpus\n",
        "        all_embs = []\n",
        "        for i in range(0, len(corpus), self.batch_size):\n",
        "            batch = corpus[i:i+self.batch_size]\n",
        "            all_embs.append(self.model.encode(batch, convert_to_tensor=True))\n",
        "        self.embeddings = torch.cat(all_embs, dim=0)\n",
        "    def retrieve(self, query, k=3):\n",
        "        q_emb = self.model.encode([query], convert_to_tensor=True)\n",
        "        sims = cosine_similarity(q_emb.cpu().numpy(), self.embeddings.cpu().numpy())[0]\n",
        "        idx = np.argsort(sims)[::-1][:k]\n",
        "        return [{'document':self.corpus[i],'score':sims[i]} for i in idx]\n",
        "\n",
        "class HybridRetriever:\n",
        "    def __init__(self, bm25: BM25Retriever, dpr: DPRRetriever, alpha: float = 0.5):\n",
        "        self.bm25 = bm25\n",
        "        self.dpr = dpr\n",
        "        self.alpha = alpha\n",
        "    def retrieve(self, query, k=3):\n",
        "        bm25_res = self.bm25.retrieve(query, k=len(self.bm25.corpus))\n",
        "        dpr_res = self.dpr.retrieve(query, k=len(self.dpr.corpus))\n",
        "        combined_scores = {}\n",
        "        for r in bm25_res:\n",
        "            combined_scores[r['document']] = combined_scores.get(r['document'],0)+(1-self.alpha)*r['score']\n",
        "        for r in dpr_res:\n",
        "            combined_scores[r['document']] = combined_scores.get(r['document'],0)+self.alpha*r['score']\n",
        "        sorted_docs = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
        "        return [{'document': doc, 'score': score} for doc, score in sorted_docs]\n",
        "\n",
        "# ================================\n",
        "# Load retrievers\n",
        "# ================================\n",
        "with open(PICKLE_FILE, \"rb\") as f:\n",
        "    retrievers = pickle.load(f)\n",
        "print(f\"‚úÖ Retrievers loaded from {PICKLE_FILE}\")\n",
        "\n",
        "# ================================\n",
        "# Load validation data\n",
        "# ================================\n",
        "val_data = []\n",
        "if DATASET_SELECTION in [\"v1\", \"both\"]:\n",
        "    squad_v1_val = load_dataset(\"squad\", split=f\"validation[:{NUM_VALIDATION_EXAMPLES}]\")\n",
        "    val_data.extend([(\"v1\", item) for item in squad_v1_val])\n",
        "\n",
        "# ================================\n",
        "# Load LLM\n",
        "# ================================\n",
        "print(\"Loading lightweight LLM (GPT-Neo 125M)...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
        "model = AutoModelForCausalLM.from_pretrained(LLM_MODEL, device_map=\"auto\")\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)  # CPU safe\n",
        "\n",
        "def generate_answer(context: str, question: str):\n",
        "    # truncate context to avoid token overflow\n",
        "    words = context.split()[:MAX_CONTEXT_TOKENS]\n",
        "    truncated_context = \" \".join(words)\n",
        "    prompt = f\"Context: {truncated_context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
        "    out = generator(prompt, max_new_tokens=200, do_sample=True, temperature=0.7)\n",
        "    return out[0]['generated_text'].split(\"Answer:\")[-1].strip()\n",
        "\n",
        "# ================================\n",
        "# Hallucination metric\n",
        "# ================================\n",
        "def hallucination_rate(answer: str, context: str):\n",
        "    ans_tokens = set(answer.lower().split())\n",
        "    ctx_tokens = set(context.lower().split())\n",
        "    if len(ans_tokens) == 0: return 1.0\n",
        "    return 1 - len(ans_tokens & ctx_tokens) / len(ans_tokens)\n",
        "\n",
        "# ================================\n",
        "# Load semantic similarity model\n",
        "# ================================\n",
        "semantic_model = SentenceTransformer(SEMANTIC_MODEL)\n",
        "rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "# ================================\n",
        "# Evaluation\n",
        "# ================================\n",
        "results = []\n",
        "\n",
        "for dataset_name, example in tqdm(val_data, desc=\"Evaluating queries with LLM\"):\n",
        "    question = example['question']\n",
        "    gold_answer = example['answers']['text'][0] if example['answers']['text'] else \"\"\n",
        "\n",
        "    for retriever_type in [\"bm25\", \"dpr\", \"hybrid\"]:\n",
        "        retriever_key = f\"{retriever_type}_{dataset_name}\"\n",
        "        retriever = retrievers.get(retriever_key)\n",
        "        if retriever:\n",
        "            retrieved = retriever.retrieve(question, k=TOP_K)\n",
        "            norm_scores = [r['score'] for r in retrieved]\n",
        "            context = \" \".join([r['document'] for r in retrieved])\n",
        "            llm_answer = generate_answer(context, question)\n",
        "            hall = hallucination_rate(llm_answer, context)\n",
        "\n",
        "            # F1\n",
        "            gold_tokens = set(gold_answer.lower().split())\n",
        "            pred_tokens = set(llm_answer.lower().split())\n",
        "            common = gold_tokens & pred_tokens\n",
        "            f1 = 2*len(common)/(len(gold_tokens)+len(pred_tokens)) if (len(gold_tokens)+len(pred_tokens)>0) else 0.0\n",
        "\n",
        "            # BLEU\n",
        "            bleu = sacrebleu.corpus_bleu([llm_answer], [[gold_answer]]).score\n",
        "\n",
        "            # ROUGE-L\n",
        "            rouge_l = rouge.score(gold_answer, llm_answer)['rougeL'].fmeasure\n",
        "\n",
        "            # Semantic similarity\n",
        "            sem_sim = cosine_similarity(\n",
        "                semantic_model.encode([gold_answer]),\n",
        "                semantic_model.encode([llm_answer])\n",
        "            )[0][0]\n",
        "\n",
        "            hit = 1 if gold_answer.lower() in llm_answer.lower() else 0\n",
        "\n",
        "            results.append({\n",
        "                \"Dataset\": dataset_name,\n",
        "                \"Retriever\": retriever_key + \"+LLM\",\n",
        "                \"Question\": question,\n",
        "                \"Gold Answer\": gold_answer,\n",
        "                \"Retrieved Context\": context[:200],\n",
        "                \"LLM Answer\": llm_answer[:200],\n",
        "                \"Score\": np.mean(norm_scores),\n",
        "                \"Hit\": hit,\n",
        "                \"Hallucination\": hall,\n",
        "                \"F1\": f1,\n",
        "                \"BLEU\": bleu,\n",
        "                \"ROUGE-L\": rouge_l,\n",
        "                \"Semantic Sim\": sem_sim\n",
        "            })\n",
        "\n",
        "# ================================\n",
        "# Convert results to DataFrame\n",
        "# ================================\n",
        "df_results = pd.DataFrame(results)\n",
        "metrics = []\n",
        "\n",
        "for retriever_key in df_results[\"Retriever\"].unique():\n",
        "    df_r = df_results[df_results[\"Retriever\"] == retriever_key]\n",
        "    total_questions = df_r[\"Question\"].nunique()\n",
        "    hits_per_question = df_r.groupby(\"Question\")[\"Hit\"].max()\n",
        "\n",
        "    metrics.append({\n",
        "        \"Retriever\": retriever_key,\n",
        "        \"Precision\": df_r[\"Hit\"].sum() / len(df_r),\n",
        "        \"Recall\": hits_per_question.sum() / total_questions,\n",
        "        \"F1\": df_r[\"F1\"].mean(),\n",
        "        \"Accuracy\": hits_per_question.sum() / total_questions,\n",
        "        \"Hallucination\": df_r[\"Hallucination\"].mean(),\n",
        "        \"BLEU\": df_r[\"BLEU\"].mean(),\n",
        "        \"ROUGE-L\": df_r[\"ROUGE-L\"].mean(),\n",
        "        \"Semantic Sim\": df_r[\"Semantic Sim\"].mean()\n",
        "    })\n",
        "\n",
        "df_metrics = pd.DataFrame(metrics)\n",
        "\n",
        "print(\"\\nüìä Retriever+LLM Metrics (Extended):\")\n",
        "print(df_metrics)\n",
        "\n",
        "# ================================\n",
        "# Save results\n",
        "# ================================\n",
        "df_results.to_csv(\"retriever_llm_results_light.csv\", index=False)\n",
        "df_metrics.to_csv(\"retriever_llm_metrics_light.csv\", index=False)\n",
        "print(\"\\n‚úÖ Results saved to CSV\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyCGdnv9RWbq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FCT7qEB1GQmA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}